{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cac9da2-9660-4702-a001-341f2445dfc7",
   "metadata": {},
   "source": [
    "Mahdollisia tarkentavia tutkimuskysymyksiä koodikirjaa ajatellen:\n",
    "\n",
    "PAIKKA: miten ympäristöä ja luontoa kuvataan, mitä elementtejä mainitaan?  \n",
    "TOIMIJAT: kenestä/mistä puhutaan (ihmiset, eläimet, ei-inhimilliset toimijat), miten vuorovaikutusta kuvataan (ollaanko yksin, seurassa)?  \n",
    "AKTIVITEETIT/TOIMINTA: mitä luonnossa tehdään, miten siellä ollaan/liikutaan?  \n",
    "TAPAHTUMAT: mistä kerrotaan, mitä havainnoinnin aikana on tapahtunut?  \n",
    "MOTIIVIT JA TAVOITTEET: miksi luonnossa ollaan, mitä siellä halutaan tehdä ja saavuttaa?  \n",
    "TUNTEET: miltä osallistujasta tuntuu ja miten hän puhuu omista tuntemuksistaan ja aistimuksistaan? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f716f37-bb32-46d4-9a4e-739f3e0be258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Start by reading the texts of interest from the file system\n",
    "\n",
    "folder = \"data/linnut\"\n",
    "prefix = \"nayte\"\n",
    "\n",
    "contents = []\n",
    "for fname in sorted(os.listdir(folder)):\n",
    "    if not fname.startswith(prefix):\n",
    "        continue\n",
    "    path = os.path.join(folder, fname)\n",
    "    with open(path, 'r') as f:\n",
    "        contents.append((fname, \"\".join(f.readlines())))\n",
    "\n",
    "# Print to check that texts are correctly read\n",
    "for fname, text in contents:\n",
    "    print(f\"{fname}:\\n\\n\")\n",
    "    print(f\"{text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df636cb4-fd68-4a6f-94fd-3a4fe44ff657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "from llm import generate_simple\n",
    "\n",
    "# Now, generate a codebook for each text. To mitigate randomness, we actually generate multiple codebooks for each text, and afterwards combine all codebooks of all texts into one codebook with clustering.\n",
    "\n",
    "# Specify a machine-readable output format for a single codebook (a list of code-explanation pairs)\n",
    "output_format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"codes\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"explanation\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"code\",\n",
    "                    \"explanation\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"codes\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# For every text, generate {n_iter} codebooks. \n",
    "n_iter = 5\n",
    "\n",
    "codelists = []\n",
    "for fname, text in contents:\n",
    "    subresults = []\n",
    "    for idx in range(n_iter):\n",
    "\n",
    "        # It is easier for llms to do the codebook in two steps: first request a free-formatted codebook and then request it in the correct format. \n",
    "        # This also allows different roles: we could use a reasoning model (which is bad at formatting) to do the first step and a formatting model (which is bad at reasoning) to do the second step.\n",
    "\n",
    "        # Define the instruction for the first step:\n",
    "        \n",
    "        instruction = \"\"\"\n",
    "        Olet laadullisen tutkimuksen avustaja. Tarkoituksesi on etsiä seuraavasta tekstistä olennaisimmat käsitteet eli koodit.\n",
    "        \n",
    "        Lue teksti huolella ja anna vastaukseksi koodit ja jokaiselle koodille jokin perustelu. Keskity tutkimuskysymykseen liittyviin koodeihin. Yritä valita vain tärkeimmät ja ytimekkäimmät koodit, suosi yksisanaisia koodeja. Vastaa suomeksi.\n",
    "        \"\"\"\n",
    "        \n",
    "        # # An alternative for a more specific question:\n",
    "        # instruction = \"\"\"\n",
    "        # Olet laadullisen tutkimuksen avustaja. Tarkoituksesi on etsiä seuraavasta tekstistä olennaisimmat käsitteet eli koodit, kuitenkin niin että ne liittyvät erityisesti seuraavaan tutkimuskysymykseen:\n",
    "        # \n",
    "        # TAPAHTUMAT: mistä kerrotaan, mitä havainnoinnin aikana on tapahtunut?\n",
    "        # \n",
    "        # Lue teksti huolella ja anna vastaukseksi koodit ja jokaiselle koodille jokin perustelu. Keskity tutkimuskysymykseen liittyviin koodeihin. Yritä valita vain tärkeimmät ja ytimekkäimmät koodit, suosi yksisanaisia koodeja. Vastaa suomeksi.\n",
    "        # \"\"\"   \n",
    "        \n",
    "        # Generate the intermediate result using the text, the instruction and a unique seed (to get a different result each time):\n",
    "        \n",
    "        model = \"llama3.3:70b\"\n",
    "        result = generate_simple(model, instruction, text, seed=idx)\n",
    "\n",
    "        # Extract the result\n",
    "        content = result['message']['content']\n",
    "\n",
    "        # Define the instruction for the formatting task:\n",
    "        \n",
    "        instruction = \"\"\"\n",
    "        Olet laadullisen tutkimuksen avustaja. Ota alla oleva vapaamuotoisesti esitetty koodikirja ja muotoile se uudestaan pyydettyyn muotoon.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the machine-readable result:\n",
    "        \n",
    "        model = \"llama3.3:70b\"\n",
    "        result = generate_simple(model, instruction, content, seed=10, output_format=output_format)\n",
    "\n",
    "        # Extract the machine-readable result.\n",
    "        codes = json.loads(result['message']['content'])['codes']\n",
    "        \n",
    "        # Store the single result.\n",
    "        subresults.append(codes)\n",
    "        \n",
    "    # Store the results of all iterations of a single text.\n",
    "    codelists.append((fname, subresults))\n",
    "\n",
    "# Now codelists variable includes {n_iter} codebooks for each of the texts. Print them to see.\n",
    "for fname, subresults in codelists:\n",
    "    print(f\"{fname}:\\n\")\n",
    "    for idx, codelist in enumerate(subresults):\n",
    "        print(f\"Iteraatio {idx+1}\\n\")\n",
    "        pprint(codelist)\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5e1a5-8f74-44c6-8758-d56d0a7b9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from llm import embed\n",
    "\n",
    "# In the following steps, we \n",
    "# 1) Flatten all the codebooks into a single list of codes\n",
    "# 2) The codes are embedded into a \"semantic space\" \n",
    "# 3) The embedded codes are clustered (put into groups) and the smallest clusters are discarded. \n",
    "# 4) The codes in a cluster are combined so that each cluster .\n",
    "# This process allows us to find the most reliable codes of all texts. Of course, this has a trade-off and manual inspection is recommended.\n",
    "\n",
    "# First, flatten the codebooks into a single list of codes.\n",
    "codes = []\n",
    "for name, textvalues in codelists:\n",
    "    for iteration in textvalues:\n",
    "        for codedict in iteration:\n",
    "            codes.append(codedict['code'].replace(\"*\", \"\"))\n",
    "\n",
    "# Embed each code into the \"semantic space\".\n",
    "vectors = []\n",
    "for code in codes:\n",
    "    result = embed(code)\n",
    "    vectors.append(result['embedding'])\n",
    "\n",
    "print(f\"{len(vectors)} codes embedded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959ba6f-8bbd-40f8-bcdc-1415869ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Then the clustering. The following code creates a visualization of the clustering as a tree and creates a user-controlled slider to change the cut-off threshold.\n",
    "# The resulting clusters are written below the plots. Here, one can experiment with different thresholds. The threshold is then set in the next step.\n",
    "\n",
    "# Helper function to calculate within-cluster sum of squares (WCSS) for a range of thresholds\n",
    "def calculate_wcss(vectors, Z, thresholds):\n",
    "    vecs = np.array(vectors)\n",
    "    wcss_values = []\n",
    "    for t in thresholds:\n",
    "        clusters = fcluster(Z, t, criterion='distance')\n",
    "        wcss = 0\n",
    "        for cluster_id in np.unique(clusters):\n",
    "            cluster_points = vecs[clusters == cluster_id]\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            wcss += np.sum(np.linalg.norm(cluster_points - centroid, axis=1)**2)\n",
    "        wcss_values.append(wcss)\n",
    "    return wcss_values\n",
    "\n",
    "# Helper function to get the rendered text based on clusters and codes\n",
    "def format_cluster_contents(clusters, codes):\n",
    "    cluster_dict = {}\n",
    "    for cluster_id, code in zip(clusters, codes):\n",
    "        if cluster_id not in cluster_dict:\n",
    "            cluster_dict[cluster_id] = []\n",
    "        cluster_dict[cluster_id].append(code)\n",
    "        \n",
    "    html = \"<div style='max-height: 400px; overflow-y: auto;'>\"\n",
    "    for cluster_id in sorted(cluster_dict.keys()):\n",
    "        items = cluster_dict[cluster_id]\n",
    "        html += f\"<p><strong>Cluster {cluster_id}</strong> ({len(items)} items):<br>\"\n",
    "        html += \", \".join(items)\n",
    "        html += \"</p>\"\n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "# Generate a static linkage-structure for the embedded codes, used in clustering\n",
    "distance_matrix = pdist(vectors, metric='cosine')\n",
    "Z = linkage(distance_matrix, method='average')\n",
    "\n",
    "# Select sensible plot params\n",
    "min_threshold, max_threshold = 0.05, 0.8\n",
    "\n",
    "# Generate the knee-plot values\n",
    "thresholds = np.linspace(min_threshold, max_threshold, num=100)\n",
    "wcss_values = calculate_wcss(vectors, Z, thresholds)\n",
    "\n",
    "# Create a user-controllable slider\n",
    "threshold_slider = widgets.FloatSlider(\n",
    "    value=min_threshold,\n",
    "    min=min_threshold,\n",
    "    max=max_threshold,\n",
    "    step=(max_threshold - min_threshold) / 100,\n",
    "    description='Threshold:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Set defaults for the rendered text    \n",
    "cluster_info = widgets.Label(value=\"Number of clusters: 0\")\n",
    "cluster_contents = widgets.HTML(value=\"\")\n",
    "\n",
    "# Create figure for the plots\n",
    "plt.ioff()\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15,10))\n",
    "ax1, ax2 = axs\n",
    "\n",
    "# This is called everytime we change the slider\n",
    "def update_plot(threshold):\n",
    "\n",
    "    # First update the tree-plot\n",
    "    ax1.clear()\n",
    "    dendrogram(Z, ax=ax1, no_labels=True)\n",
    "    ax1.axhline(y=threshold, color='r', linestyle='--')\n",
    "\n",
    "    # Then update the WCSS plot\n",
    "    ax2.clear()\n",
    "    ax2.plot(thresholds, wcss_values, label='WCSS')\n",
    "    ax2.axvline(x=threshold, color='r', linestyle='--')\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('WCSS')\n",
    "\n",
    "    # Finally, show the clusters for this specific threshold in text below the plots.\n",
    "    clusters = fcluster(Z, threshold, criterion='distance')\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    cluster_info.value = f\"Number of clusters: {n_clusters}\"\n",
    "    cluster_contents.value = format_cluster_contents(clusters, codes)\n",
    "    \n",
    "    # Ensure proper layout\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close()\n",
    "\n",
    "# Finall, create and show interactive clustering widget.\n",
    "interactive_plot = widgets.interactive(update_plot, threshold=threshold_slider)\n",
    "display(widgets.VBox([\n",
    "    threshold_slider,\n",
    "    cluster_info,\n",
    "    interactive_plot.children[-1],\n",
    "    cluster_contents\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c12286-64fc-44e4-9b44-cedb2f62e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actually apply the threshold and create the clusters.\n",
    "\n",
    "# The threshold-parameter for the clustering (bigger threshold means bigger clusters. Here, smaller values are probably better.)\n",
    "threshold = 0.28\n",
    "\n",
    "# Discard clusters with less than {min_size} elements.\n",
    "min_size = 2\n",
    "\n",
    "# Compute the clusters with the selected threshold\n",
    "distance_matrix = pdist(vectors, metric='cosine')\n",
    "Z = linkage(distance_matrix, method='average')\n",
    "clusters = fcluster(Z, threshold, criterion='distance')\n",
    "\n",
    "# Reorganize to a simple list of clusters with smallest clusters (size < {min_size}) discarded\n",
    "cluster_dict = {}\n",
    "for cluster_id, code in zip(clusters, codes):\n",
    "    if cluster_id not in cluster_dict:\n",
    "        cluster_dict[cluster_id] = []\n",
    "    cluster_dict[cluster_id].append(code)\n",
    "code_clusters = sorted([cluster[1] for cluster in cluster_dict.items() if len(cluster[1]) >= min_size], key=lambda x: len(x))\n",
    "\n",
    "# Finally print the resulting clusters to check\n",
    "for idx, cluster in enumerate(code_clusters):\n",
    "    print(f\"{idx+1}: {cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7014a-0d36-4d5d-8869-a7a5c0b1140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import generate_simple\n",
    "\n",
    "# Now we have clusters, but we actually wanted a codebook. We will ask llm to pick or create a represenative code for each of the clusters.\n",
    "\n",
    "# Specify a machine-readable output format\n",
    "output_format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"code\": {\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"code\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Go through each of the clusters, and generate a representative\n",
    "final_codes = []\n",
    "for idx, cluster in enumerate(code_clusters):\n",
    "    \n",
    "    # Define a instruction:\n",
    "    \n",
    "    instruction = \"\"\"\n",
    "    Olet laadullisen tutkimuksen avustaja. Saat syötteenä klusteroinnin seurauksena syntyneen klusterin koodit. \n",
    "    Tarkoituksesi on yhdistää annettu lista koodeja yhdeksi koodiksi kenties vain valitsemalla yksi tai sitten muuten yhdistämällä. \n",
    "    Vastauksen pitäisi olla mahdollisimman yksinkertainen.\n",
    "    \"\"\"\n",
    "\n",
    "    # For data, we set a comma-separated list of cluster elements.\n",
    "    data = \", \".join(cluster)\n",
    "\n",
    "    # Send the request to llm.\n",
    "    model = \"llama3.3:70b\"\n",
    "    result = generate_simple(model, instruction, data, seed=10, n_context=1024, output_format=output_format)\n",
    "\n",
    "    # Extract the code\n",
    "    code = json.loads(result['message']['content'])\n",
    "\n",
    "    # And store it\n",
    "    final_codes.append(code)\n",
    "\n",
    "# Print the final codebook.\n",
    "print(\"Final codes:\")\n",
    "for idx, code in enumerate(final_codes):\n",
    "    print(f\"{idx+1}: {code} ({\", \".join(code_clusters[idx])})\")\n",
    "\n",
    "print(\"\\nIn a format for easy copying:\")\n",
    "print([codedict['code'] for codedict in final_codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa64c4-51b4-4245-bb9a-84b72f6b5dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
